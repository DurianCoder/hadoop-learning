11:00:43.611 [main] INFO  com.example.hadoop.HadoopApplication - Starting HadoopApplication on JZ2019C01-0161 with PID 68376 (C:\my_workspace\hadoop\target\classes started by ying.jiang in C:\my_workspace\hadoop)
11:00:43.638 [main] INFO  com.example.hadoop.HadoopApplication - No active profile set, falling back to default profiles: default
11:00:44.717 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
11:00:44.718 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
11:00:44.719 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.33]
11:00:44.831 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
11:00:45.168 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
11:00:45.195 [main] INFO  com.example.hadoop.HadoopApplication - Started HadoopApplication in 2.545 seconds (JVM running for 3.863)
11:00:45.197 [main] INFO  com.example.hadoop.HadoopApplication - Hello world...
11:01:16.409 [main] INFO  com.example.hadoop.HadoopApplication - Starting HadoopApplication on JZ2019C01-0161 with PID 5864 (C:\my_workspace\hadoop\target\classes started by ying.jiang in C:\my_workspace\hadoop)
11:01:16.420 [main] INFO  com.example.hadoop.HadoopApplication - No active profile set, falling back to default profiles: default
11:01:17.261 [main] INFO  com.example.hadoop.HadoopApplication - Started HadoopApplication in 1.421 seconds (JVM running for 2.8)
11:01:17.263 [main] INFO  com.example.hadoop.HadoopApplication - Hello world...
11:03:30.067 [main] INFO  com.example.hadoop.HadoopApplication - Starting HadoopApplication on JZ2019C01-0161 with PID 67668 (C:\my_workspace\hadoop\target\classes started by ying.jiang in C:\my_workspace\hadoop)
11:03:30.077 [main] INFO  com.example.hadoop.HadoopApplication - No active profile set, falling back to default profiles: default
11:03:30.902 [main] INFO  com.example.hadoop.HadoopApplication - Started HadoopApplication in 1.315 seconds (JVM running for 2.743)
11:03:30.905 [main] INFO  com.example.hadoop.HadoopApplication - Hello world...
11:22:07.822 [main] INFO  com.example.hadoop.HadoopApplication - Starting HadoopApplication on JZ2019C01-0161 with PID 43684 (C:\my_workspace\hadoop\target\classes started by ying.jiang in C:\my_workspace\hadoop)
11:22:07.867 [main] INFO  com.example.hadoop.HadoopApplication - No active profile set, falling back to default profiles: default
11:22:08.861 [main] INFO  com.example.hadoop.HadoopApplication - Started HadoopApplication in 1.567 seconds (JVM running for 3.337)
11:22:08.863 [main] INFO  com.example.hadoop.HadoopApplication - Hello world...
11:22:08.866 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - hdfs file path:hdfs:/input/test.txt
11:22:08.942 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
11:22:08.951 [main] WARN  org.apache.hadoop.util.Shell - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
11:22:09.000 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
11:22:10.040 [main] INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
11:28:40.121 [main] INFO  com.example.hadoop.HadoopApplication - Starting HadoopApplication on JZ2019C01-0161 with PID 25244 (C:\my_workspace\hadoop\target\classes started by ying.jiang in C:\my_workspace\hadoop)
11:28:40.135 [main] INFO  com.example.hadoop.HadoopApplication - No active profile set, falling back to default profiles: default
11:28:40.830 [main] INFO  com.example.hadoop.HadoopApplication - Started HadoopApplication in 1.229 seconds (JVM running for 2.617)
11:28:40.832 [main] INFO  com.example.hadoop.HadoopApplication - Hello world...
11:28:40.876 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
11:28:40.877 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - hdfs file path:hdfs:/input/test.txt
11:28:40.884 [main] WARN  org.apache.hadoop.util.Shell - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
11:28:40.925 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
11:28:41.651 [main] INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
11:33:58.326 [main] INFO  com.example.hadoop.HadoopApplication - Starting HadoopApplication on JZ2019C01-0161 with PID 70084 (C:\my_workspace\hadoop\target\classes started by ying.jiang in C:\my_workspace\hadoop)
11:33:58.343 [main] INFO  com.example.hadoop.HadoopApplication - No active profile set, falling back to default profiles: default
11:33:58.959 [main] INFO  com.example.hadoop.HadoopApplication - Started HadoopApplication in 1.088 seconds (JVM running for 2.422)
11:33:58.961 [main] INFO  com.example.hadoop.HadoopApplication - Hello world...
11:33:59.009 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
11:33:59.011 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - hdfs file path:hdfs:/input/test1.txt
11:33:59.020 [main] WARN  org.apache.hadoop.util.Shell - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
11:33:59.062 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
11:33:59.823 [main] WARN  com.example.hadoop.hdfs.HdfsFileSystemUtils - hdfs文件:hdfs:/input/test1.txt不存在!
11:33:59.844 [main] ERROR com.example.hadoop.hdfs.HdfsFileSystemUtils - [401:File does not exist: /input/test1.txt
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:86)
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:76)
	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:158)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1954)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:755)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:439)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
]
11:33:59.846 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - create hdfs file path:hdfs:/input
11:33:59.856 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - 创建目录:hdfs:/input成功
11:40:09.330 [main] INFO  com.example.hadoop.HadoopApplication - Starting HadoopApplication on JZ2019C01-0161 with PID 2932 (C:\my_workspace\hadoop\target\classes started by ying.jiang in C:\my_workspace\hadoop)
11:40:09.341 [main] INFO  com.example.hadoop.HadoopApplication - No active profile set, falling back to default profiles: default
11:40:10.318 [main] INFO  com.example.hadoop.HadoopApplication - Started HadoopApplication in 1.773 seconds (JVM running for 2.976)
11:40:10.319 [main] INFO  com.example.hadoop.HadoopApplication - Hello world...
11:41:05.628 [main] INFO  com.example.hadoop.HadoopApplication - Starting HadoopApplication on JZ2019C01-0161 with PID 25216 (C:\my_workspace\hadoop\target\classes started by ying.jiang in C:\my_workspace\hadoop)
11:41:05.640 [main] INFO  com.example.hadoop.HadoopApplication - No active profile set, falling back to default profiles: default
11:41:06.230 [main] INFO  com.example.hadoop.HadoopApplication - Started HadoopApplication in 1.063 seconds (JVM running for 2.253)
11:41:06.231 [main] INFO  com.example.hadoop.HadoopApplication - Hello world...
11:41:06.277 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
11:41:06.278 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - hdfs file path:hdfs:/input/test1.txt
11:41:06.284 [main] WARN  org.apache.hadoop.util.Shell - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
11:41:06.325 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
11:41:07.012 [main] WARN  com.example.hadoop.hdfs.HdfsFileSystemUtils - hdfs文件:hdfs:/input/test1.txt不存在!
11:41:07.012 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - create hdfs file path:hdfs:/input
11:41:07.025 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - 创建目录:hdfs:/input成功
12:30:42.269 [main] INFO  com.example.hadoop.HadoopApplication - Starting HadoopApplication on JZ2019C01-0161 with PID 5888 (C:\my_workspace\hadoop\target\classes started by ying.jiang in C:\my_workspace\hadoop)
12:30:42.279 [main] INFO  com.example.hadoop.HadoopApplication - No active profile set, falling back to default profiles: default
12:30:42.844 [main] INFO  com.example.hadoop.HadoopApplication - Started HadoopApplication in 0.984 seconds (JVM running for 2.212)
12:30:42.846 [main] INFO  com.example.hadoop.HadoopApplication - Hello world...
12:30:42.894 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
12:30:42.896 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - create file file path:hdfs:/input/test1.txt
12:30:42.903 [main] WARN  org.apache.hadoop.util.Shell - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
12:30:42.947 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
12:30:43.692 [main] WARN  com.example.hadoop.hdfs.HdfsFileSystemUtils - hdfs文件:hdfs:/input/test1.txt不存在!
12:30:43.692 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - hdfs file path:hdfs:/input/test1.txt
12:30:43.696 [main] WARN  com.example.hadoop.hdfs.HdfsFileSystemUtils - hdfs文件:hdfs:/input/test1.txt不存在!
12:31:05.806 [main] INFO  com.example.hadoop.HadoopApplication - Starting HadoopApplication on JZ2019C01-0161 with PID 49156 (C:\my_workspace\hadoop\target\classes started by ying.jiang in C:\my_workspace\hadoop)
12:31:05.884 [main] INFO  com.example.hadoop.HadoopApplication - No active profile set, falling back to default profiles: default
12:31:06.420 [main] INFO  com.example.hadoop.HadoopApplication - Started HadoopApplication in 1.148 seconds (JVM running for 2.516)
12:31:06.422 [main] INFO  com.example.hadoop.HadoopApplication - Hello world...
12:31:06.465 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
12:31:06.467 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - create file file path:hdfs:/input/test1.txt
12:31:06.474 [main] WARN  org.apache.hadoop.util.Shell - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
12:31:06.514 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
12:31:07.103 [main] ERROR com.example.hadoop.hdfs.HdfsFileSystemUtils - [401:Permission denied: user=ying.jiang, access=WRITE, inode="/input":hadoop:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:255)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1879)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1863)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1822)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.resolvePathForStartFile(FSDirWriteFileOp.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2489)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2433)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:791)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:475)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
]
12:31:07.104 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - hdfs file path:hdfs:/input/test1.txt
12:31:07.118 [main] WARN  com.example.hadoop.hdfs.HdfsFileSystemUtils - hdfs文件:hdfs:/input/test1.txt不存在!
12:36:59.421 [main] INFO  com.example.hadoop.HadoopApplication - Starting HadoopApplication on JZ2019C01-0161 with PID 44920 (C:\my_workspace\hadoop\target\classes started by ying.jiang in C:\my_workspace\hadoop)
12:36:59.433 [main] INFO  com.example.hadoop.HadoopApplication - No active profile set, falling back to default profiles: default
12:37:00.049 [main] INFO  com.example.hadoop.HadoopApplication - Started HadoopApplication in 1.124 seconds (JVM running for 2.375)
12:37:00.053 [main] INFO  com.example.hadoop.HadoopApplication - Hello world...
12:37:00.099 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
12:37:00.100 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - create file file path:hdfs:/input/test1.txt
12:37:00.106 [main] WARN  org.apache.hadoop.util.Shell - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
12:37:00.145 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
12:37:00.849 [Thread-12] INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
12:37:00.934 [DataStreamer for file /input/test1.txt block BP-1382519462-192.168.66.10-1587042514818:blk_1073741836_1012] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - .
12:37:00.951 [DataStreamer for file /input/test1.txt] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - .
12:37:01.003 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - 创建文件：hdfs:/input/test1.txt成功
12:37:01.003 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - hdfs file path:hdfs:/input/test1.txt
12:37:01.035 [main] INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13:59:18.178 [main] INFO  com.example.hadoop.HadoopApplicationTests - Starting HadoopApplicationTests on JZ2019C01-0161 with PID 63672 (started by ying.jiang in C:\my_workspace\hadoop)
13:59:18.190 [main] INFO  com.example.hadoop.HadoopApplicationTests - No active profile set, falling back to default profiles: default
13:59:18.779 [main] INFO  com.example.hadoop.HadoopApplicationTests - Started HadoopApplicationTests in 0.959 seconds (JVM running for 2.796)
13:59:19.133 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
13:59:19.136 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - hdfs file path:hdfs:/input/test1.txt
13:59:19.150 [main] WARN  org.apache.hadoop.util.Shell - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
13:59:19.261 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13:59:20.350 [main] INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14:07:22.682 [main] INFO  com.example.hadoop.HadoopApplicationTests - Starting HadoopApplicationTests on JZ2019C01-0161 with PID 70984 (started by ying.jiang in C:\my_workspace\hadoop)
14:07:22.694 [main] INFO  com.example.hadoop.HadoopApplicationTests - No active profile set, falling back to default profiles: default
14:07:23.321 [main] INFO  com.example.hadoop.HadoopApplicationTests - Started HadoopApplicationTests in 1.062 seconds (JVM running for 3.238)
14:07:23.685 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
14:07:23.689 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - hdfs file path:hdfs:/input/test1.txt
14:07:23.706 [main] WARN  org.apache.hadoop.util.Shell - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
14:07:23.846 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14:07:24.930 [main] INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14:08:06.280 [main] INFO  com.example.hadoop.HadoopApplicationTests - Starting HadoopApplicationTests on JZ2019C01-0161 with PID 88720 (started by ying.jiang in C:\my_workspace\hadoop)
14:08:06.291 [main] INFO  com.example.hadoop.HadoopApplicationTests - No active profile set, falling back to default profiles: default
14:08:06.979 [main] INFO  com.example.hadoop.HadoopApplicationTests - Started HadoopApplicationTests in 1.039 seconds (JVM running for 3.544)
14:08:07.420 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
14:08:07.424 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - hdfs file path:hdfs:/input/test1.txt
14:08:07.449 [main] WARN  org.apache.hadoop.util.Shell - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
14:08:07.548 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14:08:08.531 [main] INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14:11:46.823 [main] INFO  com.example.hadoop.HadoopApplicationTests - Starting HadoopApplicationTests on JZ2019C01-0161 with PID 88132 (started by ying.jiang in C:\my_workspace\hadoop)
14:11:46.837 [main] INFO  com.example.hadoop.HadoopApplicationTests - No active profile set, falling back to default profiles: default
14:11:47.580 [main] INFO  com.example.hadoop.HadoopApplicationTests - Started HadoopApplicationTests in 1.187 seconds (JVM running for 3.54)
14:11:48.007 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
14:11:48.014 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - hdfs file path:hdfs:/input/test1.txt
14:11:48.044 [main] WARN  org.apache.hadoop.util.Shell - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
14:11:48.162 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14:11:49.431 [main] INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14:13:25.092 [main] INFO  com.example.hadoop.HadoopApplicationTests - Starting HadoopApplicationTests on JZ2019C01-0161 with PID 18212 (started by ying.jiang in C:\my_workspace\hadoop)
14:13:25.103 [main] INFO  com.example.hadoop.HadoopApplicationTests - No active profile set, falling back to default profiles: default
14:13:25.712 [main] INFO  com.example.hadoop.HadoopApplicationTests - Started HadoopApplicationTests in 0.982 seconds (JVM running for 3.62)
14:13:26.124 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
14:13:26.127 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - hdfs file path:hdfs:/input/test1.txt
14:13:26.142 [main] WARN  org.apache.hadoop.util.Shell - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
14:13:26.233 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14:13:27.351 [main] INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14:14:16.235 [main] INFO  com.example.hadoop.HadoopApplicationTests - Starting HadoopApplicationTests on JZ2019C01-0161 with PID 88952 (started by ying.jiang in C:\my_workspace\hadoop)
14:14:16.254 [main] INFO  com.example.hadoop.HadoopApplicationTests - No active profile set, falling back to default profiles: default
14:14:16.946 [main] INFO  com.example.hadoop.HadoopApplicationTests - Started HadoopApplicationTests in 1.19 seconds (JVM running for 3.571)
14:14:17.363 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
14:14:17.369 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - hdfs file path:hdfs:/input/test1.txt
14:14:17.393 [main] WARN  org.apache.hadoop.util.Shell - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
14:14:17.499 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14:14:18.538 [main] INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14:20:50.316 [main] INFO  com.example.hadoop.HadoopApplicationTests - Starting HadoopApplicationTests on JZ2019C01-0161 with PID 58084 (started by ying.jiang in C:\my_workspace\hadoop)
14:20:50.328 [main] INFO  com.example.hadoop.HadoopApplicationTests - No active profile set, falling back to default profiles: default
14:20:51.017 [main] INFO  com.example.hadoop.HadoopApplicationTests - Started HadoopApplicationTests in 1.085 seconds (JVM running for 3.353)
14:20:51.483 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
14:20:51.486 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - hdfs file path:hdfs:/input/test1.txt
14:20:51.507 [main] WARN  org.apache.hadoop.util.Shell - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
14:20:51.651 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14:20:53.048 [main] INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14:21:44.742 [main] INFO  com.example.hadoop.HdfsUtilTest - Starting HdfsUtilTest on JZ2019C01-0161 with PID 47312 (started by ying.jiang in C:\my_workspace\hadoop)
14:21:44.763 [main] INFO  com.example.hadoop.HdfsUtilTest - No active profile set, falling back to default profiles: default
14:21:45.467 [main] INFO  com.example.hadoop.HdfsUtilTest - Started HdfsUtilTest in 1.183 seconds (JVM running for 3.71)
14:21:45.892 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
14:21:45.896 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - hdfs file path:hdfs:/input/test1.txt
14:21:45.911 [main] WARN  org.apache.hadoop.util.Shell - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
14:21:46.002 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14:21:47.053 [main] INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14:24:59.460 [main] INFO  com.example.hadoop.HdfsUtilTest - Starting HdfsUtilTest on JZ2019C01-0161 with PID 88316 (started by ying.jiang in C:\my_workspace\hadoop)
14:24:59.516 [main] INFO  com.example.hadoop.HdfsUtilTest - No active profile set, falling back to default profiles: default
14:28:02.544 [main] INFO  com.example.hadoop.HdfsUtilTest - Starting HdfsUtilTest on JZ2019C01-0161 with PID 6468 (started by ying.jiang in C:\my_workspace\hadoop)
14:28:02.598 [main] INFO  com.example.hadoop.HdfsUtilTest - No active profile set, falling back to default profiles: default
14:28:04.984 [main] INFO  com.example.hadoop.HdfsUtilTest - Started HdfsUtilTest in 3.874 seconds (JVM running for 13.065)
14:28:06.373 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
14:28:06.375 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - file file path:hdfs:/input/test1.txt
14:28:06.452 [main] WARN  org.apache.hadoop.util.Shell - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
14:28:06.942 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14:28:09.136 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - false
14:28:09.141 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - true
14:28:09.142 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - supergroup
14:28:09.143 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - 134217728
14:28:09.144 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - 1587103160290
14:28:09.169 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - file file path:D:\1537864674277.pdf
14:28:09.450 [Thread-6] INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14:28:09.782 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - hdfs file path:hdfs:/input/test1.txt
14:28:09.903 [main] INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14:28:10.182 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - create hdfs file path:hdfs:/input/tmp
14:28:10.217 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - 创建目录:hdfs:/input/tmp成功
14:28:10.233 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - file file path:hdfs:/input/1.pdf
14:28:10.252 [main] ERROR com.example.hadoop.hdfs.HdfsFileSystemUtils - [401:File does not exist: hdfs:/input/1.pdf]
14:28:10.274 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - create hdfs file path:hdfs:/input/tmp/text.txt
14:28:10.334 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - 创建目录:hdfs:/input/tmp/text.txt成功
14:28:10.352 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - delete file file path:hdfs:/input/tmp/text.txt
14:33:56.569 [main] INFO  com.example.hadoop.HdfsUtilTest - Starting HdfsUtilTest on JZ2019C01-0161 with PID 72424 (started by ying.jiang in C:\my_workspace\hadoop)
14:33:56.650 [main] INFO  com.example.hadoop.HdfsUtilTest - No active profile set, falling back to default profiles: default
14:33:59.590 [main] INFO  com.example.hadoop.HdfsUtilTest - Started HdfsUtilTest in 4.367 seconds (JVM running for 16.404)
14:34:00.935 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
14:34:00.952 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - file file path:hdfs:/input/1537864674277.pdf
14:34:01.211 [main] WARN  org.apache.hadoop.util.Shell - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
14:34:01.541 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14:34:03.969 [main] ERROR com.example.hadoop.hdfs.HdfsFileSystemUtils - [401:java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems]
14:36:29.765 [main] INFO  com.example.hadoop.HdfsUtilTest - Starting HdfsUtilTest on JZ2019C01-0161 with PID 38052 (started by ying.jiang in C:\my_workspace\hadoop)
14:36:29.817 [main] INFO  com.example.hadoop.HdfsUtilTest - No active profile set, falling back to default profiles: default
14:36:31.952 [main] INFO  com.example.hadoop.HdfsUtilTest - Started HdfsUtilTest in 3.847 seconds (JVM running for 11.201)
14:36:33.385 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
14:36:33.402 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - file file path:hdfs:/input/1537864674277.pdf
14:36:33.503 [main] WARN  org.apache.hadoop.util.Shell - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
14:36:34.097 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14:36:36.018 [main] ERROR com.example.hadoop.hdfs.HdfsFileSystemUtils - [401:java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems]
15:25:36.614 [main] INFO  com.example.hadoop.HdfsUtilTest - Starting HdfsUtilTest on JZ2019C01-0161 with PID 50016 (started by ying.jiang in C:\my_workspace\hadoop)
15:25:36.646 [main] INFO  com.example.hadoop.HdfsUtilTest - No active profile set, falling back to default profiles: default
15:25:38.012 [main] INFO  com.example.hadoop.HdfsUtilTest - Started HdfsUtilTest in 2.55 seconds (JVM running for 6.97)
15:25:38.722 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
15:25:38.724 [main] INFO  com.example.hadoop.hdfs.HdfsFileSystemUtils - file file path:hdfs:/input/1537864674277.pdf
15:25:41.598 [main] INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
16:28:09.170 [main] INFO  com.example.hadoop.MapReduceTest - Starting MapReduceTest on JZ2019C01-0161 with PID 88944 (started by ying.jiang in C:\my_workspace\hadoop)
16:28:09.180 [main] INFO  com.example.hadoop.MapReduceTest - No active profile set, falling back to default profiles: default
16:28:09.766 [main] INFO  com.example.hadoop.MapReduceTest - Started MapReduceTest in 0.941 seconds (JVM running for 3.452)
16:28:10.158 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
16:28:11.249 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties
16:28:11.264 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - Scheduled Metric snapshot period at 10 second(s).
16:28:11.264 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system started
16:29:18.719 [main] INFO  com.example.hadoop.MapReduceTest - Starting MapReduceTest on JZ2019C01-0161 with PID 7660 (started by ying.jiang in C:\my_workspace\hadoop)
16:29:18.731 [main] INFO  com.example.hadoop.MapReduceTest - No active profile set, falling back to default profiles: default
16:29:19.329 [main] INFO  com.example.hadoop.MapReduceTest - Started MapReduceTest in 1.034 seconds (JVM running for 3.298)
16:29:19.655 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
16:29:20.689 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties
16:29:20.707 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - Scheduled Metric snapshot period at 10 second(s).
16:29:20.707 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system started
16:31:49.607 [main] INFO  com.example.hadoop.MapReduceTest - Starting MapReduceTest on JZ2019C01-0161 with PID 80968 (started by ying.jiang in C:\my_workspace\hadoop)
16:31:49.619 [main] INFO  com.example.hadoop.MapReduceTest - No active profile set, falling back to default profiles: default
16:31:50.201 [main] INFO  com.example.hadoop.MapReduceTest - Started MapReduceTest in 0.933 seconds (JVM running for 3.38)
16:31:50.535 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
16:31:51.524 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties
16:31:51.540 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - Scheduled Metric snapshot period at 10 second(s).
16:31:51.540 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system started
16:31:51.962 [main] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
16:31:52.010 [main] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
16:31:52.104 [main] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Cleaning up the staging area file:/tmp/hadoop/mapred/staging/ying.jiang411759997/.staging/job_local411759997_0001
16:33:12.464 [main] INFO  com.example.hadoop.MapReduceTest - Starting MapReduceTest on JZ2019C01-0161 with PID 87284 (started by ying.jiang in C:\my_workspace\hadoop)
16:33:12.486 [main] INFO  com.example.hadoop.MapReduceTest - No active profile set, falling back to default profiles: default
16:33:13.235 [main] INFO  com.example.hadoop.MapReduceTest - Started MapReduceTest in 1.197 seconds (JVM running for 3.689)
16:33:13.634 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
16:33:14.613 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties
16:33:14.626 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - Scheduled Metric snapshot period at 10 second(s).
16:33:14.627 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system started
16:33:14.954 [main] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
16:33:14.969 [main] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
16:33:15.012 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1
16:33:15.148 [main] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1
16:33:15.163 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
16:33:15.247 [main] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_local719504433_0001
16:33:15.250 [main] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Executing with tokens: []
16:33:15.515 [main] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://localhost:8080/
16:33:15.516 [main] INFO  org.apache.hadoop.mapreduce.Job - Running job: job_local719504433_0001
16:33:15.517 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter set in config null
16:33:15.531 [Thread-7] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2
16:33:15.532 [Thread-7] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
16:33:15.533 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:33:15.577 [Thread-7] WARN  org.apache.hadoop.mapred.LocalJobRunner - job_local719504433_0001
org.apache.hadoop.security.AccessControlException: Permission denied: user=ying.jiang, access=WRITE, inode="/":hadoop:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:255)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1879)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1863)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1822)
	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:59)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3233)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_221]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_221]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_221]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_221]
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121) ~[hadoop-common-3.2.1.jar:?]
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88) ~[hadoop-common-3.2.1.jar:?]
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2427) ~[hadoop-hdfs-client-3.2.1.jar:?]
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2401) ~[hadoop-hdfs-client-3.2.1.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1318) ~[hadoop-hdfs-client-3.2.1.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1315) ~[hadoop-hdfs-client-3.2.1.jar:?]
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) ~[hadoop-common-3.2.1.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1332) ~[hadoop-hdfs-client-3.2.1.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1307) ~[hadoop-hdfs-client-3.2.1.jar:?]
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275) ~[hadoop-common-3.2.1.jar:?]
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:355) ~[hadoop-mapreduce-client-core-3.2.1.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:541) [hadoop-mapreduce-client-common-3.2.1.jar:?]
Caused by: org.apache.hadoop.ipc.RemoteException: Permission denied: user=ying.jiang, access=WRITE, inode="/":hadoop:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:255)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1879)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1863)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1822)
	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:59)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3233)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545) ~[hadoop-common-3.2.1.jar:?]
	at org.apache.hadoop.ipc.Client.call(Client.java:1491) ~[hadoop-common-3.2.1.jar:?]
	at org.apache.hadoop.ipc.Client.call(Client.java:1388) ~[hadoop-common-3.2.1.jar:?]
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233) ~[hadoop-common-3.2.1.jar:?]
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118) ~[hadoop-common-3.2.1.jar:?]
	at com.sun.proxy.$Proxy75.mkdirs(Unknown Source) ~[?:?]
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:660) ~[hadoop-hdfs-client-3.2.1.jar:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_221]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_221]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_221]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_221]
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422) ~[hadoop-common-3.2.1.jar:?]
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165) ~[hadoop-common-3.2.1.jar:?]
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157) ~[hadoop-common-3.2.1.jar:?]
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) ~[hadoop-common-3.2.1.jar:?]
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359) ~[hadoop-common-3.2.1.jar:?]
	at com.sun.proxy.$Proxy76.mkdirs(Unknown Source) ~[?:?]
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2425) ~[hadoop-hdfs-client-3.2.1.jar:?]
	... 9 more
16:33:16.529 [main] INFO  org.apache.hadoop.mapreduce.Job - Job job_local719504433_0001 running in uber mode : false
16:33:16.529 [main] INFO  org.apache.hadoop.mapreduce.Job -  map 0% reduce 0%
16:33:16.532 [main] INFO  org.apache.hadoop.mapreduce.Job - Job job_local719504433_0001 failed with state FAILED due to: NA
16:33:16.575 [main] INFO  org.apache.hadoop.mapreduce.Job - Counters: 0
16:40:09.449 [main] INFO  com.example.hadoop.MapReduceTest - Starting MapReduceTest on JZ2019C01-0161 with PID 38856 (started by ying.jiang in C:\my_workspace\hadoop)
16:40:09.462 [main] INFO  com.example.hadoop.MapReduceTest - No active profile set, falling back to default profiles: default
16:40:10.083 [main] INFO  com.example.hadoop.MapReduceTest - Started MapReduceTest in 0.984 seconds (JVM running for 3.567)
16:40:10.567 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
16:40:11.650 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties
16:40:11.670 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - Scheduled Metric snapshot period at 10 second(s).
16:40:11.670 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system started
16:40:12.018 [main] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
16:40:12.033 [main] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
16:40:12.077 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1
16:40:12.141 [main] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1
16:40:12.152 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
16:40:12.214 [main] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_local426282884_0001
16:40:12.216 [main] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Executing with tokens: []
16:40:12.352 [main] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://localhost:8080/
16:40:12.353 [main] INFO  org.apache.hadoop.mapreduce.Job - Running job: job_local426282884_0001
16:40:12.354 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter set in config null
16:40:12.361 [Thread-7] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2
16:40:12.362 [Thread-7] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
16:40:12.362 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:40:12.388 [Thread-7] WARN  org.apache.hadoop.mapred.LocalJobRunner - job_local426282884_0001
org.apache.hadoop.security.AccessControlException: Permission denied: user=ying.jiang, access=WRITE, inode="/":hadoop:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:255)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1879)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1863)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1822)
	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:59)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3233)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_221]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_221]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_221]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_221]
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121) ~[hadoop-common-3.2.1.jar:?]
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88) ~[hadoop-common-3.2.1.jar:?]
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2427) ~[hadoop-hdfs-client-3.2.1.jar:?]
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2401) ~[hadoop-hdfs-client-3.2.1.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1318) ~[hadoop-hdfs-client-3.2.1.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1315) ~[hadoop-hdfs-client-3.2.1.jar:?]
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) ~[hadoop-common-3.2.1.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1332) ~[hadoop-hdfs-client-3.2.1.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1307) ~[hadoop-hdfs-client-3.2.1.jar:?]
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275) ~[hadoop-common-3.2.1.jar:?]
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:355) ~[hadoop-mapreduce-client-core-3.2.1.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:541) [hadoop-mapreduce-client-common-3.2.1.jar:?]
Caused by: org.apache.hadoop.ipc.RemoteException: Permission denied: user=ying.jiang, access=WRITE, inode="/":hadoop:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:255)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1879)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1863)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1822)
	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:59)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3233)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545) ~[hadoop-common-3.2.1.jar:?]
	at org.apache.hadoop.ipc.Client.call(Client.java:1491) ~[hadoop-common-3.2.1.jar:?]
	at org.apache.hadoop.ipc.Client.call(Client.java:1388) ~[hadoop-common-3.2.1.jar:?]
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233) ~[hadoop-common-3.2.1.jar:?]
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118) ~[hadoop-common-3.2.1.jar:?]
	at com.sun.proxy.$Proxy75.mkdirs(Unknown Source) ~[?:?]
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:660) ~[hadoop-hdfs-client-3.2.1.jar:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_221]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_221]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_221]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_221]
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422) ~[hadoop-common-3.2.1.jar:?]
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165) ~[hadoop-common-3.2.1.jar:?]
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157) ~[hadoop-common-3.2.1.jar:?]
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) ~[hadoop-common-3.2.1.jar:?]
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359) ~[hadoop-common-3.2.1.jar:?]
	at com.sun.proxy.$Proxy76.mkdirs(Unknown Source) ~[?:?]
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2425) ~[hadoop-hdfs-client-3.2.1.jar:?]
	... 9 more
16:40:13.358 [main] INFO  org.apache.hadoop.mapreduce.Job - Job job_local426282884_0001 running in uber mode : false
16:40:13.360 [main] INFO  org.apache.hadoop.mapreduce.Job -  map 0% reduce 0%
16:40:13.362 [main] INFO  org.apache.hadoop.mapreduce.Job - Job job_local426282884_0001 failed with state FAILED due to: NA
16:40:13.385 [main] INFO  org.apache.hadoop.mapreduce.Job - Counters: 0
16:41:15.775 [main] INFO  com.example.hadoop.MapReduceTest - Starting MapReduceTest on JZ2019C01-0161 with PID 36976 (started by ying.jiang in C:\my_workspace\hadoop)
16:41:15.788 [main] INFO  com.example.hadoop.MapReduceTest - No active profile set, falling back to default profiles: default
16:41:16.363 [main] INFO  com.example.hadoop.MapReduceTest - Started MapReduceTest in 0.936 seconds (JVM running for 3.536)
16:41:16.728 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
16:41:17.884 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties
16:41:17.898 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - Scheduled Metric snapshot period at 10 second(s).
16:41:17.899 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system started
16:41:18.222 [main] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
16:41:18.235 [main] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
16:41:18.278 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1
16:41:18.354 [main] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1
16:41:18.364 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
16:41:18.433 [main] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_local1547675130_0001
16:41:18.436 [main] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Executing with tokens: []
16:41:18.576 [main] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://localhost:8080/
16:41:18.577 [main] INFO  org.apache.hadoop.mapreduce.Job - Running job: job_local1547675130_0001
16:41:18.579 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter set in config null
16:41:18.586 [Thread-7] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2
16:41:18.587 [Thread-7] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
16:41:18.588 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:41:18.670 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for map tasks
16:41:18.672 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local1547675130_0001_m_000000_0
16:41:18.711 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2
16:41:18.712 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
16:41:18.758 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.yarn.util.ProcfsBasedProcessTree - ProcfsBasedProcessTree currently is supported only on Linux.
16:41:19.032 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@1d7dd18a
16:41:19.051 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: hdfs://centos01:9000/input/test.txt:0+88
16:41:19.097 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)
16:41:19.097 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100
16:41:19.098 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080
16:41:19.098 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600
16:41:19.098 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600
16:41:19.101 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
16:41:19.143 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
16:41:19.319 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output
16:41:19.364 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - map task executor complete.
16:41:19.372 [Thread-7] WARN  org.apache.hadoop.mapred.LocalJobRunner - job_local1547675130_0001
java.lang.Exception: java.io.IOException: Type mismatch in key from map: expected org.apache.hadoop.io.Text, received org.apache.hadoop.io.LongWritable
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:492) ~[hadoop-mapreduce-client-common-3.2.1.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:552) [hadoop-mapreduce-client-common-3.2.1.jar:?]
Caused by: java.io.IOException: Type mismatch in key from map: expected org.apache.hadoop.io.Text, received org.apache.hadoop.io.LongWritable
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1088) ~[hadoop-mapreduce-client-core-3.2.1.jar:?]
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727) ~[hadoop-mapreduce-client-core-3.2.1.jar:?]
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89) ~[hadoop-mapreduce-client-core-3.2.1.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112) ~[hadoop-mapreduce-client-core-3.2.1.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125) ~[hadoop-mapreduce-client-core-3.2.1.jar:?]
	at com.example.hadoop.mapreduce.WordCount$TokenizerMapper.map(WordCount.java:83) ~[classes/:?]
	at com.example.hadoop.mapreduce.WordCount$TokenizerMapper.map(WordCount.java:77) ~[classes/:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146) ~[hadoop-mapreduce-client-core-3.2.1.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) ~[hadoop-mapreduce-client-core-3.2.1.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) ~[hadoop-mapreduce-client-core-3.2.1.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) ~[hadoop-mapreduce-client-common-3.2.1.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
16:41:19.584 [main] INFO  org.apache.hadoop.mapreduce.Job - Job job_local1547675130_0001 running in uber mode : false
16:41:19.585 [main] INFO  org.apache.hadoop.mapreduce.Job -  map 0% reduce 0%
16:41:19.588 [main] INFO  org.apache.hadoop.mapreduce.Job - Job job_local1547675130_0001 failed with state FAILED due to: NA
16:41:19.597 [main] INFO  org.apache.hadoop.mapreduce.Job - Counters: 0
16:45:38.228 [main] INFO  com.example.hadoop.MapReduceTest - Starting MapReduceTest on JZ2019C01-0161 with PID 88328 (started by ying.jiang in C:\my_workspace\hadoop)
16:45:38.242 [main] INFO  com.example.hadoop.MapReduceTest - No active profile set, falling back to default profiles: default
16:45:39.014 [main] INFO  com.example.hadoop.MapReduceTest - Started MapReduceTest in 1.336 seconds (JVM running for 3.978)
16:45:39.380 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
16:45:40.604 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties
16:45:40.619 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - Scheduled Metric snapshot period at 10 second(s).
16:45:40.619 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system started
16:46:10.794 [main] INFO  com.example.hadoop.MapReduceTest - Starting MapReduceTest on JZ2019C01-0161 with PID 15172 (started by ying.jiang in C:\my_workspace\hadoop)
16:46:10.810 [main] INFO  com.example.hadoop.MapReduceTest - No active profile set, falling back to default profiles: default
16:46:11.420 [main] INFO  com.example.hadoop.MapReduceTest - Started MapReduceTest in 0.995 seconds (JVM running for 3.503)
16:46:11.763 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
16:46:12.716 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties
16:46:12.733 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - Scheduled Metric snapshot period at 10 second(s).
16:46:12.734 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system started
16:46:13.054 [main] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
16:46:13.071 [main] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
16:46:13.115 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1
16:46:13.175 [main] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1
16:46:13.186 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
16:46:13.263 [main] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_local1113935990_0001
16:46:13.265 [main] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Executing with tokens: []
16:46:13.403 [main] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://localhost:8080/
16:46:13.404 [main] INFO  org.apache.hadoop.mapreduce.Job - Running job: job_local1113935990_0001
16:46:13.405 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter set in config null
16:46:13.412 [Thread-7] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2
16:46:13.412 [Thread-7] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
16:46:13.413 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:46:13.449 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for map tasks
16:46:13.450 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local1113935990_0001_m_000000_0
16:46:13.473 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2
16:46:13.474 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
16:46:13.485 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.yarn.util.ProcfsBasedProcessTree - ProcfsBasedProcessTree currently is supported only on Linux.
16:46:13.638 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@7a77d9a1
16:46:13.645 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: hdfs://centos01:9000/input/test.txt:0+88
16:46:13.670 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)
16:46:13.671 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100
16:46:13.671 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080
16:46:13.671 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600
16:46:13.671 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600
16:46:13.674 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
16:46:13.695 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
16:46:13.856 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output
16:46:13.875 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - map task executor complete.
16:46:13.880 [Thread-7] WARN  org.apache.hadoop.mapred.LocalJobRunner - job_local1113935990_0001
java.lang.Exception: java.io.IOException: Type mismatch in key from map: expected org.apache.hadoop.io.Text, received org.apache.hadoop.io.LongWritable
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:492) ~[hadoop-mapreduce-client-common-3.2.1.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:552) [hadoop-mapreduce-client-common-3.2.1.jar:?]
Caused by: java.io.IOException: Type mismatch in key from map: expected org.apache.hadoop.io.Text, received org.apache.hadoop.io.LongWritable
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1088) ~[hadoop-mapreduce-client-core-3.2.1.jar:?]
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727) ~[hadoop-mapreduce-client-core-3.2.1.jar:?]
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89) ~[hadoop-mapreduce-client-core-3.2.1.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112) ~[hadoop-mapreduce-client-core-3.2.1.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125) ~[hadoop-mapreduce-client-core-3.2.1.jar:?]
	at com.example.hadoop.mapreduce.WordCount$TokenizerMapper.map(WordCount.java:83) ~[classes/:?]
	at com.example.hadoop.mapreduce.WordCount$TokenizerMapper.map(WordCount.java:77) ~[classes/:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146) ~[hadoop-mapreduce-client-core-3.2.1.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) ~[hadoop-mapreduce-client-core-3.2.1.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) ~[hadoop-mapreduce-client-core-3.2.1.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) ~[hadoop-mapreduce-client-common-3.2.1.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
16:46:14.410 [main] INFO  org.apache.hadoop.mapreduce.Job - Job job_local1113935990_0001 running in uber mode : false
16:46:14.410 [main] INFO  org.apache.hadoop.mapreduce.Job -  map 0% reduce 0%
16:46:14.412 [main] INFO  org.apache.hadoop.mapreduce.Job - Job job_local1113935990_0001 failed with state FAILED due to: NA
16:46:14.419 [main] INFO  org.apache.hadoop.mapreduce.Job - Counters: 0
16:47:07.925 [main] INFO  com.example.hadoop.MapReduceTest - Starting MapReduceTest on JZ2019C01-0161 with PID 34348 (started by ying.jiang in C:\my_workspace\hadoop)
16:47:07.938 [main] INFO  com.example.hadoop.MapReduceTest - No active profile set, falling back to default profiles: default
16:47:08.531 [main] INFO  com.example.hadoop.MapReduceTest - Started MapReduceTest in 0.957 seconds (JVM running for 3.058)
16:47:08.874 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
16:47:09.886 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties
16:47:09.906 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - Scheduled Metric snapshot period at 10 second(s).
16:47:09.907 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system started
16:47:10.219 [main] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
16:47:10.234 [main] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
16:47:10.280 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1
16:47:10.342 [main] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1
16:47:10.351 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
16:47:10.412 [main] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_local1917736764_0001
16:47:10.414 [main] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Executing with tokens: []
16:47:10.549 [main] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://localhost:8080/
16:47:10.550 [main] INFO  org.apache.hadoop.mapreduce.Job - Running job: job_local1917736764_0001
16:47:10.558 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter set in config null
16:47:10.565 [Thread-7] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2
16:47:10.565 [Thread-7] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
16:47:10.566 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:47:10.606 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for map tasks
16:47:10.606 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local1917736764_0001_m_000000_0
16:47:10.631 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2
16:47:10.631 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
16:47:10.642 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.yarn.util.ProcfsBasedProcessTree - ProcfsBasedProcessTree currently is supported only on Linux.
16:47:10.790 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6a785c59
16:47:10.797 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: hdfs://centos01:9000/input/test.txt:0+88
16:47:10.864 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)
16:47:10.864 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100
16:47:10.864 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080
16:47:10.864 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600
16:47:10.865 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600
16:47:10.869 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
16:47:10.888 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
16:47:11.061 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - 
16:47:11.063 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output
16:47:11.064 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output
16:47:11.064 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufend = 159; bufvoid = 104857600
16:47:11.064 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396(104857584); kvend = 26214328(104857312); length = 69/6553600
16:47:11.191 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 0
16:47:11.216 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local1917736764_0001_m_000000_0 is done. And is in the process of committing
16:47:11.221 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map
16:47:11.221 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local1917736764_0001_m_000000_0' done.
16:47:11.229 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Final Counters for attempt_local1917736764_0001_m_000000_0: Counters: 24
	File System Counters
		FILE: Number of bytes read=162
		FILE: Number of bytes written=357953
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=88
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=5
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=1
		HDFS: Number of bytes read erasure-coded=0
	Map-Reduce Framework
		Map input records=4
		Map output records=18
		Map output bytes=159
		Map output materialized bytes=135
		Input split bytes=100
		Combine input records=18
		Combine output records=12
		Spilled Records=12
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=330301440
	File Input Format Counters 
		Bytes Read=88
16:47:11.229 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local1917736764_0001_m_000000_0
16:47:11.230 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - map task executor complete.
16:47:11.234 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for reduce tasks
16:47:11.234 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local1917736764_0001_r_000000_0
16:47:11.272 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2
16:47:11.273 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
16:47:11.274 [pool-8-thread-1] INFO  org.apache.hadoop.yarn.util.ProcfsBasedProcessTree - ProcfsBasedProcessTree currently is supported only on Linux.
16:47:11.436 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@ffd7e6f
16:47:11.439 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.ReduceTask - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@33f49f70
16:47:11.441 [pool-8-thread-1] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!
16:47:11.458 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - MergerManager: memoryLimit=2658192640, maxSingleShuffleLimit=664548160, mergeThreshold=1754407168, ioSortFactor=10, memToMemMergeOutputsThreshold=10
16:47:11.464 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - attempt_local1917736764_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
16:47:11.541 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher - localfetcher#1 about to shuffle output of map attempt_local1917736764_0001_m_000000_0 decomp: 131 len: 135 to MEMORY
16:47:11.553 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput - Read 131 bytes from map-output for attempt_local1917736764_0001_m_000000_0
16:47:11.560 [main] INFO  org.apache.hadoop.mapreduce.Job - Job job_local1917736764_0001 running in uber mode : false
16:47:11.562 [main] INFO  org.apache.hadoop.mapreduce.Job -  map 100% reduce 0%
16:47:11.618 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - closeInMemoryFile -> map-output of size: 131, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->131
16:47:11.620 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - EventFetcher is interrupted.. Returning
16:47:11.620 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.
16:47:11.621 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
16:47:11.665 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 1 sorted segments
16:47:11.665 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 1 segments left of total size: 125 bytes
16:47:11.668 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merged 1 segments, 131 bytes to disk to satisfy reduce memory limit
16:47:11.669 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 1 files, 135 bytes from disk
16:47:11.670 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 0 segments, 0 bytes from memory into reduce
16:47:11.670 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 1 sorted segments
16:47:11.672 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 1 segments left of total size: 125 bytes
16:47:11.673 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.
16:47:11.917 [pool-8-thread-1] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
16:47:12.167 [Thread-17] INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
16:47:12.948 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local1917736764_0001_r_000000_0 is done. And is in the process of committing
16:47:12.951 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.
16:47:12.952 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Task - Task attempt_local1917736764_0001_r_000000_0 is allowed to commit now
16:47:13.045 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_local1917736764_0001_r_000000_0' to hdfs://centos01:9000/output
16:47:13.046 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce
16:47:13.047 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local1917736764_0001_r_000000_0' done.
16:47:13.047 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Task - Final Counters for attempt_local1917736764_0001_r_000000_0: Counters: 30
	File System Counters
		FILE: Number of bytes read=464
		FILE: Number of bytes written=358088
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=88
		HDFS: Number of bytes written=81
		HDFS: Number of read operations=10
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=3
		HDFS: Number of bytes read erasure-coded=0
	Map-Reduce Framework
		Combine input records=0
		Combine output records=0
		Reduce input groups=12
		Reduce shuffle bytes=135
		Reduce input records=12
		Reduce output records=12
		Spilled Records=12
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=12
		Total committed heap usage (bytes)=330825728
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Output Format Counters 
		Bytes Written=81
16:47:13.048 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local1917736764_0001_r_000000_0
16:47:13.048 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce task executor complete.
16:47:13.577 [main] INFO  org.apache.hadoop.mapreduce.Job -  map 100% reduce 100%
16:47:13.578 [main] INFO  org.apache.hadoop.mapreduce.Job - Job job_local1917736764_0001 completed successfully
16:47:13.585 [main] INFO  org.apache.hadoop.mapreduce.Job - Counters: 36
	File System Counters
		FILE: Number of bytes read=626
		FILE: Number of bytes written=716041
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=176
		HDFS: Number of bytes written=81
		HDFS: Number of read operations=15
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
		HDFS: Number of bytes read erasure-coded=0
	Map-Reduce Framework
		Map input records=4
		Map output records=18
		Map output bytes=159
		Map output materialized bytes=135
		Input split bytes=100
		Combine input records=18
		Combine output records=12
		Reduce input groups=12
		Reduce shuffle bytes=135
		Reduce input records=12
		Reduce output records=12
		Spilled Records=24
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=12
		Total committed heap usage (bytes)=661127168
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=88
	File Output Format Counters 
		Bytes Written=81
16:51:55.330 [main] INFO  com.example.hadoop.MapReduceTest - Starting MapReduceTest on JZ2019C01-0161 with PID 81988 (started by ying.jiang in C:\my_workspace\hadoop)
16:51:55.342 [main] INFO  com.example.hadoop.MapReduceTest - No active profile set, falling back to default profiles: default
16:51:56.043 [main] INFO  com.example.hadoop.MapReduceTest - Started MapReduceTest in 1.074 seconds (JVM running for 3.589)
16:51:56.421 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
16:53:13.060 [main] INFO  com.example.hadoop.MapReduceTest - Starting MapReduceTest on JZ2019C01-0161 with PID 47268 (started by ying.jiang in C:\my_workspace\hadoop)
16:53:13.069 [main] INFO  com.example.hadoop.MapReduceTest - No active profile set, falling back to default profiles: default
16:53:13.655 [main] INFO  com.example.hadoop.MapReduceTest - Started MapReduceTest in 0.926 seconds (JVM running for 3.33)
16:53:13.988 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
16:57:00.702 [main] INFO  com.example.hadoop.MapReduceTest - Starting MapReduceTest on JZ2019C01-0161 with PID 68852 (started by ying.jiang in C:\my_workspace\hadoop)
16:57:00.714 [main] INFO  com.example.hadoop.MapReduceTest - No active profile set, falling back to default profiles: default
16:57:01.393 [main] INFO  com.example.hadoop.MapReduceTest - Started MapReduceTest in 1.033 seconds (JVM running for 3.457)
16:57:01.765 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
16:57:02.926 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties
16:57:02.941 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - Scheduled Metric snapshot period at 10 second(s).
16:57:02.941 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system started
16:57:03.369 [main] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
16:57:03.412 [main] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
16:57:03.448 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1
16:57:03.531 [main] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1
16:57:03.542 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
16:57:03.612 [main] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_local1780206012_0001
16:57:03.615 [main] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Executing with tokens: []
16:57:03.760 [main] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://localhost:8080/
16:57:03.764 [main] INFO  org.apache.hadoop.mapreduce.Job - Running job: job_local1780206012_0001
16:57:03.766 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter set in config null
16:57:03.773 [Thread-7] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2
16:57:03.775 [Thread-7] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
16:57:03.777 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:57:03.833 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for map tasks
16:57:03.834 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local1780206012_0001_m_000000_0
16:57:03.862 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2
16:57:03.862 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
16:57:03.874 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.yarn.util.ProcfsBasedProcessTree - ProcfsBasedProcessTree currently is supported only on Linux.
16:57:04.013 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@65482e26
16:57:04.019 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: hdfs://centos01:9000/input/test.txt:0+88
16:57:04.048 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)
16:57:04.048 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100
16:57:04.049 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080
16:57:04.049 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600
16:57:04.049 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600
16:57:04.055 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
16:57:04.089 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
16:57:04.502 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - 
16:57:04.504 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output
16:57:04.504 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output
16:57:04.504 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufend = 159; bufvoid = 104857600
16:57:04.504 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396(104857584); kvend = 26214328(104857312); length = 69/6553600
16:57:04.521 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 0
16:57:04.537 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local1780206012_0001_m_000000_0 is done. And is in the process of committing
16:57:04.541 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map
16:57:04.542 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local1780206012_0001_m_000000_0' done.
16:57:04.549 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Final Counters for attempt_local1780206012_0001_m_000000_0: Counters: 24
	File System Counters
		FILE: Number of bytes read=162
		FILE: Number of bytes written=357953
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=88
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
		HDFS: Number of bytes read erasure-coded=0
	Map-Reduce Framework
		Map input records=4
		Map output records=18
		Map output bytes=159
		Map output materialized bytes=135
		Input split bytes=100
		Combine input records=18
		Combine output records=12
		Spilled Records=12
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=223
		Total committed heap usage (bytes)=543686656
	File Input Format Counters 
		Bytes Read=88
16:57:04.550 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local1780206012_0001_m_000000_0
16:57:04.552 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - map task executor complete.
16:57:04.554 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for reduce tasks
16:57:04.557 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local1780206012_0001_r_000000_0
16:57:04.566 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2
16:57:04.566 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
16:57:04.567 [pool-8-thread-1] INFO  org.apache.hadoop.yarn.util.ProcfsBasedProcessTree - ProcfsBasedProcessTree currently is supported only on Linux.
16:57:04.698 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6179c9dd
16:57:04.701 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.ReduceTask - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6b255728
16:57:04.702 [pool-8-thread-1] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!
16:57:04.727 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - MergerManager: memoryLimit=2658192640, maxSingleShuffleLimit=664548160, mergeThreshold=1754407168, ioSortFactor=10, memToMemMergeOutputsThreshold=10
16:57:04.752 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - attempt_local1780206012_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
16:57:04.780 [main] INFO  org.apache.hadoop.mapreduce.Job - Job job_local1780206012_0001 running in uber mode : false
16:57:04.790 [main] INFO  org.apache.hadoop.mapreduce.Job -  map 100% reduce 0%
16:57:04.793 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher - localfetcher#1 about to shuffle output of map attempt_local1780206012_0001_m_000000_0 decomp: 131 len: 135 to MEMORY
16:57:04.805 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput - Read 131 bytes from map-output for attempt_local1780206012_0001_m_000000_0
16:57:04.809 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - closeInMemoryFile -> map-output of size: 131, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->131
16:57:04.811 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - EventFetcher is interrupted.. Returning
16:57:04.813 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.
16:57:04.814 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
16:57:04.829 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 1 sorted segments
16:57:04.829 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 1 segments left of total size: 125 bytes
16:57:04.831 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merged 1 segments, 131 bytes to disk to satisfy reduce memory limit
16:57:04.833 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 1 files, 135 bytes from disk
16:57:04.834 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 0 segments, 0 bytes from memory into reduce
16:57:04.834 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 1 sorted segments
16:57:04.836 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 1 segments left of total size: 125 bytes
16:57:04.836 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.
16:57:04.861 [pool-8-thread-1] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
16:57:04.886 [Thread-17] INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
16:57:05.245 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local1780206012_0001_r_000000_0 is done. And is in the process of committing
16:57:05.248 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.
16:57:05.248 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Task - Task attempt_local1780206012_0001_r_000000_0 is allowed to commit now
16:57:05.270 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_local1780206012_0001_r_000000_0' to hdfs://centos01:9000/output
16:57:05.272 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce
16:57:05.272 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local1780206012_0001_r_000000_0' done.
16:57:05.273 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Task - Final Counters for attempt_local1780206012_0001_r_000000_0: Counters: 30
	File System Counters
		FILE: Number of bytes read=464
		FILE: Number of bytes written=358088
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=88
		HDFS: Number of bytes written=81
		HDFS: Number of read operations=11
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
		HDFS: Number of bytes read erasure-coded=0
	Map-Reduce Framework
		Combine input records=0
		Combine output records=0
		Reduce input groups=12
		Reduce shuffle bytes=135
		Reduce input records=12
		Reduce output records=12
		Spilled Records=12
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=543686656
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Output Format Counters 
		Bytes Written=81
16:57:05.273 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local1780206012_0001_r_000000_0
16:57:05.275 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce task executor complete.
16:57:05.792 [main] INFO  org.apache.hadoop.mapreduce.Job -  map 100% reduce 100%
16:57:05.792 [main] INFO  org.apache.hadoop.mapreduce.Job - Job job_local1780206012_0001 completed successfully
16:57:05.801 [main] INFO  org.apache.hadoop.mapreduce.Job - Counters: 36
	File System Counters
		FILE: Number of bytes read=626
		FILE: Number of bytes written=716041
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=176
		HDFS: Number of bytes written=81
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=6
		HDFS: Number of bytes read erasure-coded=0
	Map-Reduce Framework
		Map input records=4
		Map output records=18
		Map output bytes=159
		Map output materialized bytes=135
		Input split bytes=100
		Combine input records=18
		Combine output records=12
		Reduce input groups=12
		Reduce shuffle bytes=135
		Reduce input records=12
		Reduce output records=12
		Spilled Records=24
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=223
		Total committed heap usage (bytes)=1087373312
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=88
	File Output Format Counters 
		Bytes Written=81
17:04:03.215 [main] INFO  com.example.hadoop.MapReduceTest - Starting MapReduceTest on JZ2019C01-0161 with PID 69312 (started by ying.jiang in C:\my_workspace\hadoop)
17:04:03.241 [main] INFO  com.example.hadoop.MapReduceTest - No active profile set, falling back to default profiles: default
17:04:04.051 [main] INFO  com.example.hadoop.MapReduceTest - Started MapReduceTest in 1.42 seconds (JVM running for 5.399)
17:04:04.516 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
17:04:05.648 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties
17:04:05.665 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - Scheduled Metric snapshot period at 10 second(s).
17:04:05.666 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system started
17:04:05.953 [main] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
17:04:05.968 [main] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
17:04:06.004 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1
17:04:06.081 [main] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1
17:04:06.094 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
17:04:06.194 [main] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_local1448131975_0001
17:04:06.197 [main] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Executing with tokens: []
17:04:06.335 [main] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://localhost:8080/
17:04:06.337 [main] INFO  org.apache.hadoop.mapreduce.Job - Running job: job_local1448131975_0001
17:04:06.339 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter set in config null
17:04:06.347 [Thread-7] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2
17:04:06.348 [Thread-7] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
17:04:06.349 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17:04:06.395 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for map tasks
17:04:06.400 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local1448131975_0001_m_000000_0
17:04:06.428 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2
17:04:06.428 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
17:04:06.440 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.yarn.util.ProcfsBasedProcessTree - ProcfsBasedProcessTree currently is supported only on Linux.
17:04:06.600 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6577402e
17:04:06.645 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: hdfs://centos01:9000/input/test.txt:0+88
17:04:06.678 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)
17:04:06.678 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100
17:04:06.678 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080
17:04:06.679 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600
17:04:06.679 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600
17:04:06.684 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
17:04:06.708 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
17:04:22.396 [main] INFO  org.apache.hadoop.mapreduce.Job - Job job_local1448131975_0001 running in uber mode : false
17:04:22.398 [main] INFO  org.apache.hadoop.mapreduce.Job -  map 0% reduce 0%
17:04:22.399 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - 
17:04:22.401 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output
17:04:22.402 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output
17:04:22.402 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufend = 159; bufvoid = 104857600
17:04:22.402 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396(104857584); kvend = 26214328(104857312); length = 69/6553600
17:04:22.426 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 0
17:04:22.442 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local1448131975_0001_m_000000_0 is done. And is in the process of committing
17:04:22.451 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map
17:04:22.451 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local1448131975_0001_m_000000_0' done.
17:04:22.460 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Final Counters for attempt_local1448131975_0001_m_000000_0: Counters: 24
	File System Counters
		FILE: Number of bytes read=162
		FILE: Number of bytes written=357953
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=88
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
		HDFS: Number of bytes read erasure-coded=0
	Map-Reduce Framework
		Map input records=4
		Map output records=18
		Map output bytes=159
		Map output materialized bytes=135
		Input split bytes=100
		Combine input records=18
		Combine output records=12
		Spilled Records=12
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=196
		Total committed heap usage (bytes)=559415296
	File Input Format Counters 
		Bytes Read=88
17:04:22.461 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local1448131975_0001_m_000000_0
17:04:22.463 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - map task executor complete.
17:04:22.467 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for reduce tasks
17:04:22.468 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local1448131975_0001_r_000000_0
17:04:22.477 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2
17:04:22.478 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
17:04:22.479 [pool-8-thread-1] INFO  org.apache.hadoop.yarn.util.ProcfsBasedProcessTree - ProcfsBasedProcessTree currently is supported only on Linux.
17:04:22.636 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@63271d3d
17:04:22.640 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.ReduceTask - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@668000fa
17:04:22.642 [pool-8-thread-1] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!
17:04:22.659 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - MergerManager: memoryLimit=2658192640, maxSingleShuffleLimit=664548160, mergeThreshold=1754407168, ioSortFactor=10, memToMemMergeOutputsThreshold=10
17:04:22.662 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - attempt_local1448131975_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
17:04:22.684 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher - localfetcher#1 about to shuffle output of map attempt_local1448131975_0001_m_000000_0 decomp: 131 len: 135 to MEMORY
17:04:22.689 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput - Read 131 bytes from map-output for attempt_local1448131975_0001_m_000000_0
17:04:22.691 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - closeInMemoryFile -> map-output of size: 131, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->131
17:04:22.692 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - EventFetcher is interrupted.. Returning
17:04:22.694 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.
17:04:22.694 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
17:04:22.706 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 1 sorted segments
17:04:22.706 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 1 segments left of total size: 125 bytes
17:04:22.709 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merged 1 segments, 131 bytes to disk to satisfy reduce memory limit
17:04:22.710 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 1 files, 135 bytes from disk
17:04:22.710 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 0 segments, 0 bytes from memory into reduce
17:04:22.711 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 1 sorted segments
17:04:22.713 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 1 segments left of total size: 125 bytes
17:04:22.713 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.
17:04:22.739 [pool-8-thread-1] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
17:04:22.777 [Thread-18] INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
17:04:23.129 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local1448131975_0001_r_000000_0 is done. And is in the process of committing
17:04:23.132 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.
17:04:23.133 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Task - Task attempt_local1448131975_0001_r_000000_0 is allowed to commit now
17:04:23.181 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_local1448131975_0001_r_000000_0' to hdfs://centos01:9000/output
17:04:23.182 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce
17:04:23.182 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local1448131975_0001_r_000000_0' done.
17:04:23.183 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Task - Final Counters for attempt_local1448131975_0001_r_000000_0: Counters: 30
	File System Counters
		FILE: Number of bytes read=464
		FILE: Number of bytes written=358088
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=88
		HDFS: Number of bytes written=81
		HDFS: Number of read operations=11
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
		HDFS: Number of bytes read erasure-coded=0
	Map-Reduce Framework
		Combine input records=0
		Combine output records=0
		Reduce input groups=12
		Reduce shuffle bytes=135
		Reduce input records=12
		Reduce output records=12
		Spilled Records=12
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=559415296
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Output Format Counters 
		Bytes Written=81
17:04:23.183 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local1448131975_0001_r_000000_0
17:04:23.183 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce task executor complete.
17:04:23.400 [main] INFO  org.apache.hadoop.mapreduce.Job -  map 100% reduce 100%
17:04:23.401 [main] INFO  org.apache.hadoop.mapreduce.Job - Job job_local1448131975_0001 completed successfully
17:04:23.410 [main] INFO  org.apache.hadoop.mapreduce.Job - Counters: 36
	File System Counters
		FILE: Number of bytes read=626
		FILE: Number of bytes written=716041
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=176
		HDFS: Number of bytes written=81
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=6
		HDFS: Number of bytes read erasure-coded=0
	Map-Reduce Framework
		Map input records=4
		Map output records=18
		Map output bytes=159
		Map output materialized bytes=135
		Input split bytes=100
		Combine input records=18
		Combine output records=12
		Reduce input groups=12
		Reduce shuffle bytes=135
		Reduce input records=12
		Reduce output records=12
		Spilled Records=24
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=196
		Total committed heap usage (bytes)=1118830592
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=88
	File Output Format Counters 
		Bytes Written=81
17:24:48.552 [main] INFO  com.example.hadoop.MapReduceTest - Starting MapReduceTest on JZ2019C01-0161 with PID 37008 (started by ying.jiang in C:\my_workspace\hadoop)
17:24:48.569 [main] INFO  com.example.hadoop.MapReduceTest - No active profile set, falling back to default profiles: default
17:24:49.259 [main] INFO  com.example.hadoop.MapReduceTest - Started MapReduceTest in 1.083 seconds (JVM running for 4.79)
17:24:49.720 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
17:24:50.802 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties
17:24:50.819 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - Scheduled Metric snapshot period at 10 second(s).
17:24:50.820 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system started
17:24:51.077 [main] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
17:24:51.091 [main] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
17:24:51.122 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1
17:24:51.184 [main] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1
17:24:51.194 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
17:24:51.256 [main] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_local1631477976_0001
17:24:51.259 [main] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Executing with tokens: []
17:24:51.398 [main] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://localhost:8080/
17:24:51.399 [main] INFO  org.apache.hadoop.mapreduce.Job - Running job: job_local1631477976_0001
17:24:51.401 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter set in config null
17:24:51.411 [Thread-7] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2
17:24:51.411 [Thread-7] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
17:24:51.412 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17:24:51.461 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for map tasks
17:24:51.462 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local1631477976_0001_m_000000_0
17:24:51.486 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2
17:24:51.486 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
17:24:51.496 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.yarn.util.ProcfsBasedProcessTree - ProcfsBasedProcessTree currently is supported only on Linux.
17:24:51.636 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@3f17009e
17:24:51.645 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: hdfs://centos01:9000/input/test.txt:0+88
17:24:51.677 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)
17:24:51.677 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100
17:24:51.678 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080
17:24:51.678 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600
17:24:51.678 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600
17:24:51.682 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
17:24:51.712 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
17:24:51.872 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - 
17:24:51.875 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output
17:24:51.875 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output
17:24:51.875 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufend = 159; bufvoid = 104857600
17:24:51.875 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396(104857584); kvend = 26214328(104857312); length = 69/6553600
17:24:51.895 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 0
17:24:51.918 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local1631477976_0001_m_000000_0 is done. And is in the process of committing
17:24:51.925 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map
17:24:51.925 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local1631477976_0001_m_000000_0' done.
17:24:51.936 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Final Counters for attempt_local1631477976_0001_m_000000_0: Counters: 24
	File System Counters
		FILE: Number of bytes read=162
		FILE: Number of bytes written=357953
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=88
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
		HDFS: Number of bytes read erasure-coded=0
	Map-Reduce Framework
		Map input records=4
		Map output records=18
		Map output bytes=159
		Map output materialized bytes=135
		Input split bytes=100
		Combine input records=18
		Combine output records=12
		Spilled Records=12
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=318242816
	File Input Format Counters 
		Bytes Read=88
17:24:51.936 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local1631477976_0001_m_000000_0
17:24:51.940 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - map task executor complete.
17:24:51.942 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for reduce tasks
17:24:51.943 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local1631477976_0001_r_000000_0
17:24:51.950 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2
17:24:51.950 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
17:24:51.951 [pool-8-thread-1] INFO  org.apache.hadoop.yarn.util.ProcfsBasedProcessTree - ProcfsBasedProcessTree currently is supported only on Linux.
17:24:52.084 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@4cd9b4e7
17:24:52.087 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.ReduceTask - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7fb48e23
17:24:52.090 [pool-8-thread-1] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!
17:24:52.112 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - MergerManager: memoryLimit=2658192640, maxSingleShuffleLimit=664548160, mergeThreshold=1754407168, ioSortFactor=10, memToMemMergeOutputsThreshold=10
17:24:52.121 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - attempt_local1631477976_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
17:24:52.143 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher - localfetcher#1 about to shuffle output of map attempt_local1631477976_0001_m_000000_0 decomp: 131 len: 135 to MEMORY
17:24:52.147 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput - Read 131 bytes from map-output for attempt_local1631477976_0001_m_000000_0
17:24:52.150 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - closeInMemoryFile -> map-output of size: 131, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->131
17:24:52.151 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - EventFetcher is interrupted.. Returning
17:24:52.153 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.
17:24:52.153 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
17:24:52.173 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 1 sorted segments
17:24:52.174 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 1 segments left of total size: 125 bytes
17:24:52.177 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merged 1 segments, 131 bytes to disk to satisfy reduce memory limit
17:24:52.178 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 1 files, 135 bytes from disk
17:24:52.179 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 0 segments, 0 bytes from memory into reduce
17:24:52.179 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 1 sorted segments
17:24:52.181 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 1 segments left of total size: 125 bytes
17:24:52.181 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.
17:24:52.206 [pool-8-thread-1] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
17:24:52.229 [Thread-17] INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
17:24:52.406 [main] INFO  org.apache.hadoop.mapreduce.Job - Job job_local1631477976_0001 running in uber mode : false
17:24:52.411 [main] INFO  org.apache.hadoop.mapreduce.Job -  map 100% reduce 0%
17:24:52.505 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local1631477976_0001_r_000000_0 is done. And is in the process of committing
17:24:52.513 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.
17:24:52.513 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Task - Task attempt_local1631477976_0001_r_000000_0 is allowed to commit now
17:24:52.537 [pool-8-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_local1631477976_0001_r_000000_0' to hdfs://centos01:9000/output
17:24:52.540 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce
17:24:52.541 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local1631477976_0001_r_000000_0' done.
17:24:52.541 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.Task - Final Counters for attempt_local1631477976_0001_r_000000_0: Counters: 30
	File System Counters
		FILE: Number of bytes read=464
		FILE: Number of bytes written=358088
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=88
		HDFS: Number of bytes written=81
		HDFS: Number of read operations=11
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
		HDFS: Number of bytes read erasure-coded=0
	Map-Reduce Framework
		Combine input records=0
		Combine output records=0
		Reduce input groups=12
		Reduce shuffle bytes=135
		Reduce input records=12
		Reduce output records=12
		Spilled Records=12
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=329252864
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Output Format Counters 
		Bytes Written=81
17:24:52.545 [pool-8-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local1631477976_0001_r_000000_0
17:24:52.546 [Thread-7] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce task executor complete.
17:24:53.414 [main] INFO  org.apache.hadoop.mapreduce.Job -  map 100% reduce 100%
17:24:53.415 [main] INFO  org.apache.hadoop.mapreduce.Job - Job job_local1631477976_0001 completed successfully
17:24:53.423 [main] INFO  org.apache.hadoop.mapreduce.Job - Counters: 36
	File System Counters
		FILE: Number of bytes read=626
		FILE: Number of bytes written=716041
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=176
		HDFS: Number of bytes written=81
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=6
		HDFS: Number of bytes read erasure-coded=0
	Map-Reduce Framework
		Map input records=4
		Map output records=18
		Map output bytes=159
		Map output materialized bytes=135
		Input split bytes=100
		Combine input records=18
		Combine output records=12
		Reduce input groups=12
		Reduce shuffle bytes=135
		Reduce input records=12
		Reduce output records=12
		Spilled Records=24
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=647495680
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=88
	File Output Format Counters 
		Bytes Written=81
